# 损失函数
## 回归损失函数
$$
MSE = \frac{1}{N}\sum_{i=1}^N\left(y_i-y_i^p\right)^2
$$

$$
MAE = \frac{1}{N} \sum_{i=1}^{N} |y_i - y_i^p|
$$

mse收敛快，且随着误差减小，梯度也在减小；mae大部分情况下梯度相等，不利于函数收敛且在 $ y - f(x) = 0 $处不可导。
mse对离群点敏感；mae则不会

## 分类损失函数
### 二分类交叉熵损失（可通过最大似然估计获得）
$$
L = -\frac{1}{N} \sum_{i=1}^{N} [y_i \log(p_i) + (1-y_i) \log(1-p_i)]
$$

描述真实分布与预测分布之间的距离
二分类不用mse是因为梯度消失，并且交叉熵更关注正确类别的预测概率
### 多分类交叉熵损失
$$
L = -\frac{1}{N} \sum_{i=1}^{N} \sum_{c=1}^{C} y_{i,c} \log(p_{i,c})
$$

## 偏差-方差权衡
误差：模型预测结果与实际值之间的差异（主要目标）
偏差：模型预测的平均误差
方差：模型预测值的离散程度

## PCA
将n维特征映射到k维（主成分）
### 特征值分解
中心化——计算协方差矩阵 $\frac{1}{n} X X^{T}$ ——选择最大的前k个特征值，并用特征向量构造投影矩阵P—— $Y = PX$
### SVD分解
中心化——对X做SVD分解——计算协方差矩阵——...
几乎总是用SVD：数值稳定性高，计算效率高

## 数据不均衡
考虑正例很少，负例很多的解决方法：
欠采样：对负例进行欠采样。一种代表性算法是将负例分为很多份，每次用其中一份和正例一起训练，最后用集成学习综合结果

过采样：对正例进行过采样。一种代表性方法是对正例进行线性插值来获得更多的正例

调整损失函数：训练时正常训练，分类时将数据不平衡问题加入到决策过程中。通过加入权值使得数量较少的正样本得到更多的关注，不至于被大量的负样本掩盖

组合/集成学习：例如正负样本比例1:100，则将负样本分为100份，正样本每次有放回采样保持与负样本数相同，然后取100次结果进行平均

数据增强：单样本增强如几何变换、颜色变换、增加噪声；多样本组合增强如Mixup、SamplePairing等，在特征空间内构造已知样本的邻域值样本；基于深度学习数据增强

## 特征选择
### 过滤式（统计属性，简单迅速但会忽略特征相互关系）
方差选择：计算特征在数据中的方差来判断是否保留，方差大保留

相关系数：计算特征与目标变量之间的相互关系

卡方检验：适用于分类问题中的特征选择。计算特征与目标变量之间的卡方统计量 $\chi^2 = \sum_{i=1}^{n} \frac{(O_i - E_i)^2}{E_i}$  O是观察频数，E是期望频数

互信息：

$$
I(X; Y) = \sum_{x \in X} \sum_{y \in Y} P(x, y) \log \left( \frac{P(x, y)}{P(x) P(y)} \right)
$$ 

衡量两个变量之间相关性的指标。0的时候独立，非负。

### 嵌入式
正则化（惩罚项）、决策树剪枝
### 包裹式
在特征子集上进行交叉验证，例如决策树和随机森林

递归特征消除RFE：训练模型——评估重要性得分——剔除最差的特征——递归重复——直到达到预设特征数量

遗传算法：生成随机特征子集———计算每个子集的模型性能——筛选优秀个体——交叉组合父代特征值——变异特征——直到连续多代无提升

## 逻辑回归
 $\sigma(z) = \frac{1}{1 + e^{-z}}$，它将输出映射到(0,1)区间。

 ## 特征工程
特征使用：基于理解找出对因变量有影响的自变量；可用性评估（获取难度、准确率）
 
特征获取：如何获取（数据库、API、爬虫）/储存（格式与位置）
### 特征处理
特征清洗：异常值/采样

预处理：单个特征（归一化，离散化，缺失值，数据变化）；多个特征（PCA、LDA降维；特征选择）

特征监控：有效性分析；提取重要特征

### 树模型进行特征工程
改善模型性能（组合特征）；降低过拟合风险（降维）；减少计算复杂度；提高可解释性；处理特征相关性

## AdaBoost
初始化权重1/N——训练弱分类器，赋予错误样本大权重，迭代训练——组合弱学习器





