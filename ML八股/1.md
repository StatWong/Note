# 损失函数
## 回归损失函数
$$
MSE = \frac{1}{N}\sum_{i=1}^N\left(y_i-y_i^p\right)^2
$$

$$
MAE = \frac{1}{N} \sum_{i=1}^{N} |y_i - y_i^p|
$$

mse收敛快，且随着误差减小，梯度也在减小；mae大部分情况下梯度相等，不利于函数收敛且在 $ y - f(x) = 0 $处不可导。
mse对离群点敏感；mae则不会

## 分类损失函数
### 二分类交叉熵损失（可通过最大似然估计获得）
$$
L = -\frac{1}{N} \sum_{i=1}^{N} [y_i \log(p_i) + (1-y_i) \log(1-p_i)]
$$

描述真实分布与预测分布之间的距离
二分类不用mse是因为梯度消失，并且交叉熵更关注正确类别的预测概率
### 多分类交叉熵损失
$$
L = -\frac{1}{N} \sum_{i=1}^{N} \sum_{c=1}^{C} y_{i,c} \log(p_{i,c})
$$

## 偏差-方差权衡
误差：模型预测结果与实际值之间的差异（主要目标）
偏差：模型预测的平均误差
方差：模型预测值的离散程度

## PCA
将n维特征映射到k维（主成分）
### 特征值分解
中心化——计算协方差矩阵 $\frac{1}{n} X X^{T}$ ——选择最大的前k个特征值，并用特征向量构造投影矩阵P—— $Y = PX$
### SVD分解
中心化——对X做SVD分解——计算协方差矩阵——...
几乎总是用SVD：数值稳定性高，计算效率高

## 数据不均衡
考虑正例很少，负例很多的解决方法：
欠采样：对负例进行欠采样。一种代表性算法是将负例分为很多份，每次用其中一份和正例一起训练，最后用集成学习综合结果

过采样：对正例进行过采样。一种代表性方法是对正例进行线性插值来获得更多的正例

调整损失函数：训练时正常训练，分类时将数据不平衡问题加入到决策过程中。通过加入权值使得数量较少的正样本得到更多的关注，不至于被大量的负样本掩盖

组合/集成学习：例如正负样本比例1:100，则将负样本分为100份，正样本每次有放回采样保持与负样本数相同，然后取100次结果进行平均

数据增强：单样本增强如几何变换、颜色变换、增加噪声；多样本组合增强如Mixup、SamplePairing等，在特征空间内构造已知样本的邻域值样本；基于深度学习数据增强

## 特征选择
### 过滤式（统计属性，简单迅速但会忽略特征相互关系）
方差选择：计算特征在数据中的方差来判断是否保留，方差大保留

相关系数：计算特征与目标变量之间的相互关系

卡方检验：适用于分类问题中的特征选择。计算特征与目标变量之间的卡方统计量 $\chi^2 = \sum_{i=1}^{n} \frac{(O_i - E_i)^2}{E_i}$  O是观察频数，E是期望频数

互信息：

$$
I(X; Y) = \sum_{x \in X} \sum_{y \in Y} P(x, y) \log \left( \frac{P(x, y)}{P(x) P(y)} \right)
$$ 

衡量两个变量之间相关性的指标。0的时候独立，非负。

### 嵌入式
正则化（惩罚项）、决策树剪枝
### 包裹式
在特征子集上进行交叉验证，例如决策树和随机森林

递归特征消除RFE：训练模型——评估重要性得分——剔除最差的特征——递归重复——直到达到预设特征数量

遗传算法：生成随机特征子集———计算每个子集的模型性能——筛选优秀个体——交叉组合父代特征值——变异特征——直到连续多代无提升

## 逻辑回归
 $\sigma(z) = \frac{1}{1 + e^{-z}}$，它将输出映射到(0,1)区间。

 ## 特征工程
特征使用：基于理解找出对因变量有影响的自变量；可用性评估（获取难度、准确率）
 
特征获取：如何获取（数据库、API、爬虫）/储存（格式与位置）
### 特征处理
特征清洗：异常值/采样

预处理：单个特征（归一化，离散化，缺失值，数据变化）；多个特征（PCA、LDA降维；特征选择）

特征监控：有效性分析；提取重要特征

### 树模型进行特征工程
改善模型性能（组合特征）；降低过拟合风险（降维）；减少计算复杂度；提高可解释性；处理特征相关性

## boosting（顺序集成）
### AdaBoost
初始化权重1/N——训练弱分类器，赋予错误样本大权重，迭代训练——组合弱学习器，加大分类误差率小的弱分类器的权重

### GBDT（梯度提升树）
CART回归树：loss采用均方误差，因为每次拟合的是梯度值，连续的
Gradient Boosting：拟合负残差，每一个后续的模型会把前面没有拟合好的残差（负梯度，因为损失是MSE）重新拟合

## 逻辑回归和梯度提升树
| 对比维度 | LR（Logistic Regression） | GBDT（Gradient Boosting Decision Tree） |
|---------|--------------------------|----------------------------------------|
| **用途** | 分类模型 | 既可分类又可回归 |
| **损失函数** | 交叉熵损失 | 采用回归拟合，用当前损失拟合实际值与预测值之间的残差 |
| **正则化** | 采用 L1 和 L2 正则 | 通过弱分类器个数（迭代轮次 T）控制复杂度 |
| **特征组合** | 线性模型，解释性强，易并行化，但学习能力有限，需要大量特征工程 | 可处理线性和非线性数据，具有天然特征组合优势 |
| **学习能力** | 线性模型，学习能力有限 | 非线性模型，学习能力强 |
| **并行化** | 容易并行化 | 训练过程串行，难以并行 |
| **解释性** | 模型解释性强 | 模型解释性较弱 |

## 随机森林和梯度提升树
| 对比维度 | RF（随机森林） | GBDT（梯度提升决策树） |
|---------|---------------|----------------------|
| **树类型** | 可以是分类树或回归树 | 只由回归树组成 |
| **集成思想** | Bagging（并行集成） | Boosting（串行集成） |
| **并行性** | 树可以并行生成 | 只能串行生成 |
| **最终结果** | 投票机制（分类）或平均（回归） | 将所有树的结果累加 |
| **数据敏感性** | 对异常值不敏感 | 对异常值非常敏感 |
| **训练样本** | 有放回抽样（Bootstrap） | 每次使用全部样本 |
| **偏差-方差权衡** | 主要减少方差 | 主要减少偏差 |
| **过拟合倾向** | 相对不易过拟合 | 更容易过拟合 |

## 感知机模型

$$ f(x) = \text{sign}(w \cdot x + b) $$

其中：sign(x) = 1 if x >= 0 else x < 0

| 对比维度 | 感知机（Perceptron） | 逻辑回归（Logistic Regression） |
|---------|---------------------|--------------------------------|
| **激活函数** | 阶跃函数（sign函数） | Sigmoid函数 |
| **输出类型** | 离散值（-1, +1） | 概率值（0-1之间） |
| **损失函数** | 误分类点到超平面的距离之和 | 交叉熵损失函数 |
| **应用场景** | 二分类 | 二分类或多分类 |
| **输出解释** | 直接输出类别标签 | 输出属于某类的概率 |

## 推荐算法
协同过滤： 分析用户的兴趣和行为，利用人以类聚、物以群分的思想做

内容过滤：从物品或者内容的特征出发，从相关性的角度推荐

组合推荐

实现思路：将用户和物品表示为特征向量，通过计算相似度来预测喜好程度（余弦相似度、欧几里得距离、皮尔逊相关系数），最后根据用户对电影的评分与预测喜好程度计算均方误差作为损失






