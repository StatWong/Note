# DL 八股
## 激活函数
非线性，可微性（梯度下降法），单调性（损失凸函数）
### sigmoid
$\sigma(x) = \frac{1}{1 + e^{-x}}$

值域在0到1之间，到数值在0到0.25之间，连续可导

缺点：x在无穷大或者无穷小时，导数为0，梯度消失

### Tanh函数

$$
tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$

最大梯度为1，缓解梯度消失的问题，但当x过大/小时依旧会梯度消失；幂运算，计算量大

### ReLU函数

$$
\text{ReLU}(x) = \begin{cases} 
x & \text{if } x > 0 \\
0 & \text{otherwise}
\end{cases}
$$

正区间解决梯度消失问题；线性计算快；负区间梯度消失

### leaky relu
负区间取为ax，a为小于1的常数

## softmax函数及求导

$$
S_i = \frac{e^{a_i}}{\sum_{k=1}^{K} e^{a_k}}
$$

a是原始得分，概率和为1

## 优化器
### BGD：批量梯度下降

$$
\theta = \theta - \eta \cdot \nabla_\theta J(\theta)
$$

全局最优解，但训练很慢

### SGD：随机梯度下降法

$$
\theta = \theta - \eta \cdot \nabla_\theta J(\theta; x^{(i)}, y^{(i)})
$$

实现简单，效率高，收敛慢，局部最小值

### MDGD：小批次梯度下降

$$
\theta = \theta - \eta \cdot \frac{1}{b} \sum_{i=k}^{k+b-1} \nabla_\theta J(\theta; x^{(i)}, y^{(i)})
$$

每次选取b个作为训练，凸函数全局最优

### AdaGrad：自适应梯度优化器

$$
\begin{aligned}
g_t &= \nabla_\theta J(\theta_t) \\
G_t &= G_{t-1} + g_t \odot g_t \\
\theta_{t+1} &= \theta_t - \frac{\eta}{\sqrt{G_t + \epsilon}} \odot g_t
\end{aligned}
$$

平缓处学习率大，陡峭处学习率小，依赖之前的所有梯度

### RMSProp

$$
\begin{aligned}
g_t &= \nabla_\theta J(\theta_t) \\
E[g^2]_t &= \rho E[g^2]_{t-1} + (1 - \rho) g_t \odot g_t \\
\theta_{t+1} &= \theta_t - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}} \odot g_t
\end{aligned}
$$

调整权重，比较依赖当前梯度，更前的梯度影响降低

### Adam：自适应动量估计

$$
\begin{aligned}
m_t &= \beta_1 m_{t-1} + (1 - \beta_1) g_t \\
v_t &= \beta_2 v_{t-1} + (1 - \beta_2) g_t \odot g_t \\
\hat{m}_t &= \frac{m_t}{1 - \beta_1^t} \\
\hat{v}_t &= \frac{v_t}{1 - \beta_2^t} \\
\theta_{t+1} &= \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t
\end{aligned}
$$

## 残差链接
跳跃连接，将输入和输出直接连接：避免梯度消失或梯度爆炸；简化优化问题；提高模型表达能力（特别是多尺度特征问题，例如图像识别）
