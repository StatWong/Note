# DL 八股
## 激活函数
权重参数做线性变化；激活函数做非线性变化

非线性，可微性（梯度下降法），单调性（损失凸函数）
### sigmoid
$\sigma(x) = \frac{1}{1 + e^{-x}}$

值域在0到1之间，到数值在0到0.25之间，连续可导

缺点：x在无穷大或者无穷小时，导数为0，梯度消失

### Tanh函数

$$
tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$

最大梯度为1，缓解梯度消失的问题，但当x过大/小时依旧会梯度消失；幂运算，计算量大

### ReLU函数

$$
\text{ReLU}(x) = \begin{cases} 
x & \text{if } x > 0 \\
0 & \text{otherwise}
\end{cases}
$$

正区间解决梯度消失问题；线性计算快；负区间梯度消失

### leaky relu
负区间取为ax，a为小于1的常数

## softmax函数及求导

$$
S_i = \frac{e^{a_i}}{\sum_{k=1}^{K} e^{a_k}}
$$

a是原始得分，概率和为1

## 优化器
### BGD：批量梯度下降

$$
\theta = \theta - \eta \cdot \nabla_\theta J(\theta)
$$

全局最优解，但训练很慢

### SGD：随机梯度下降法

$$
\theta = \theta - \eta \cdot \nabla_\theta J(\theta; x^{(i)}, y^{(i)})
$$

实现简单，效率高，收敛慢，局部最小值

### MDGD：小批次梯度下降

$$
\theta = \theta - \eta \cdot \frac{1}{b} \sum_{i=k}^{k+b-1} \nabla_\theta J(\theta; x^{(i)}, y^{(i)})
$$

每次选取b个作为训练，凸函数全局最优

### AdaGrad：自适应梯度优化器

$$
\begin{aligned}
g_t &= \nabla_\theta J(\theta_t) \\
G_t &= G_{t-1} + g_t \odot g_t \\
\theta_{t+1} &= \theta_t - \frac{\eta}{\sqrt{G_t + \epsilon}} \odot g_t
\end{aligned}
$$

平缓处学习率大，陡峭处学习率小，依赖之前的所有梯度

### RMSProp

$$
\begin{aligned}
g_t &= \nabla_\theta J(\theta_t) \\
E[g^2]_t &= \rho E[g^2]_{t-1} + (1 - \rho) g_t \odot g_t \\
\theta_{t+1} &= \theta_t - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}} \odot g_t
\end{aligned}
$$

调整权重，比较依赖当前梯度，更前的梯度影响降低

### Adam：自适应动量估计

$$
\begin{aligned}
m_t &= \beta_1 m_{t-1} + (1 - \beta_1) g_t \\
v_t &= \beta_2 v_{t-1} + (1 - \beta_2) g_t \odot g_t \\
\hat{m}_t &= \frac{m_t}{1 - \beta_1^t} \\
\hat{v}_t &= \frac{v_t}{1 - \beta_2^t} \\
\theta_{t+1} &= \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t
\end{aligned}
$$

## 残差链接
跳跃连接，将输入和输出直接连接：避免梯度消失或梯度爆炸；简化优化问题；提高模型表达能力（特别是多尺度特征问题，例如图像识别）

## 梯度消失
原因：隐藏曾层数过多；采用了不合适的激活函数（sigmoid和tanh）

解决方法：Relu或leaky Relu；Batch Normalization将输入值放在[-1,1]；残差连接(ResNet:残差映射 F(x) = H(x) - x);LSTM通过门控机制有选择地传递信息
## 梯度爆炸
原因：隐藏层过多；初始权重设置得太大

解决方法：权重正则化；梯度裁剪；Batch Normalization

## 归一化
最大最小值： $$x' = \frac{x - \min(x)}{\max(x) - \min(x)}$$  保持原始分布；异常值敏感

零一均值归一化： $$x' = \frac{x - \mu}{\delta}$$ 异常值不敏感，保持数据分布；会在[-1,1]之外

Softmax： $$x' = \frac{e^x}{\sum e^x}$$ 输出具备概率解释；计算成本高

## 评价指标
Accuracy,precision,recall,f1-score

### unweighted
UF1： $$UF1 = \frac{\sum_{i}^{C} UF1_i}{C}$$  

UAR： $$UAR = \frac{\sum_{i}^{C} Recall_i}{C}$$

## 防止过拟合
dropout：每次训练，随机部分神经元不更新

增大数据量；早停：每个epoch后用验证集计算loss，如果先降后升则表明可能过拟合

批量归一化：加快训练并且在训练时对小批量数据引入噪声（均值方差通过部分样本估计），迫使模型学习更鲁棒的特征

正则化：L1让权重快速趋于0，意味着一些神经元的作用被完全抑制，减少非必要特征；L2通过限制权重的大小，使函数平滑

## Batch Normalization和Layer Normalization
增强网络泛化性能；加快训练速度

BN: 跨样本，对同一特征归一化，用于CV

LN： 跨特征，对同一样本归一化，用于NLP（用BN会扰乱非padding填充的特征）

## 全连接层作用
特征融合；分类


## 神经网络与树模型
| 特性         | 深度学习模型                                      | 树模型                                          |
|--------------|--------------------------------------------------|-------------------------------------------------|
| **优点**     |                                                  |                                                 |
|              | - 擅长处理复杂数据（大规模、高维、非线性）         | - 易解释性强，特征重要性直观                    |
|              | - 特征学习能力强，自动提取特征                     | - 对少量数据或质量差的数据表现好                |
|              | - 泛化能力强（在大数据集上）                       | - 训练和预测速度快，适合中小规模数据            |
| **缺点**     |                                                  |                                                 |
|              | - 需要大量数据和计算资源                           | - 容易过拟合（尤其树深大或未剪枝）              |
|              | - 需要仔细调参和优化结构                           | - 不擅长处理复杂非线性关系                      |
|              | - 可解释性差                                     | - 通常需要人工特征工程                          |






